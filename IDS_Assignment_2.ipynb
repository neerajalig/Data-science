{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "IDS_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neerajalig/Data-science/blob/main/IDS_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE3bPVsy44S5"
      },
      "source": [
        "# The Second Part of the Assignment of IDS 2020-2021\n",
        "In this Jupyter notebook, please, document your results and the way you have obtained them. Please use the _Python environment_ provided at the beginning of the course. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing all datasets and other outputs you have generated (such as pdf, jpg, and others). Please make sure that the datasets and other outputs are easily identifiable, i.e. use names as requested in the corresponding question.\n",
        "\n",
        "This is the _only_ submission that is required (Jupyter notebook + zip-file). A separate report is _not_ needed and will not be considered for grading. \n",
        "\n",
        "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
        "\n",
        "It is not needed that the group members be the same as the group members of the first part of the assignment, <font color=\"red\"> *Please make sure to include names and matriculation numbers of all group members in the slot provided below.* </font> If a name or a student's matriculation number is missing, the student will not receive any points.\n",
        "\n",
        "Hint 1: While working on the assignment, you will get a better understanding of the datasets. Feel free to generate additional results and visualizations to support your answers. For example, this might be useful regarding data modification and simplification. <font color=\"red\">Ensure that all your claims are supported.</font>\n",
        "\n",
        "Hint 2: <font color=\"red\">Plan your time wisely. </font> A few parts of this assignment might take some time to run. It might be necessary to consider time management when you plan your group work.\n",
        "\n",
        "Hint 3: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-bHBLWlnh3w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjQcc0-l44TP"
      },
      "source": [
        "<font color=\"red\"><b>Student Names and matriculation numbers:\n",
        "    \n",
        "    1. Kshitij Dubey, 414331\n",
        "    \n",
        "    2. Sparsh Jauhari, 414325\n",
        "    \n",
        "    3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiP4ORpI44TR"
      },
      "source": [
        "## Question 1 - Data Preprocessing and Data Quality (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r170HEXz44TS"
      },
      "source": [
        "(a) Carry out the following preprocessing steps before starting the analysis:\n",
        "\n",
        "Select 90% of dataset <b>dataPrepViz.csv</b> for this assignment by random sampling. Use the matriculation number of one of the group members as seed. Rename the sampled dataset to <b>dataPrepViz_sampled</b> and export it as CSV.\n",
        "\n",
        " - <font color='red'>Important!</font> Make sure that you submit your extracted dataset with your results in Moodle.\n",
        "\n",
        "Use this dataset <b>dataPrepViz_sampled</b> as starting point for Question 1 and Question 2. Then apply further modifications as specified in the those questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j1Aar1e44TU"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "# Read csv file\n",
        "df = pd.read_csv(r\"IDS_Assignment2/DataFiles/dataPrepViz.csv\",sep=';')\n",
        "\n",
        "# Random sampling the data\n",
        "dsSampled = df.sample(frac=0.9, random_state = 414331)\n",
        "\n",
        "# Print data to csv file\n",
        "dsSampled.to_csv(r\"IDS_Assignment2/DataFiles/dataPrepViz_sampled.csv\",index=False,header=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8rdftMJ44TV"
      },
      "source": [
        "Create new dataset <b>data1</b> by removing the feature 'geographic_group' from <b>dataPrepViz_sampled</b>. Use this <b>data1</b> dataset for Question 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzc3MTmi44TW"
      },
      "source": [
        "data1 = dsSampled.drop(['geographic_group'], axis=1)\n",
        "print(data1.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN_sP-se44TX"
      },
      "source": [
        "(b) We want to get a first impression of the data. To achieve this, compute and show the following:\n",
        "\n",
        "- the column names (the names of the features)\n",
        "- the data type of each feature\n",
        "- for categorical features: the number of classes and the value of the most frequent class\n",
        "- for numerical features: the mean, standard deviation, minimum and maximum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlsZTsZv44TY"
      },
      "source": [
        "import numpy as np\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "print('Column names & data type of features:\\n')\n",
        "print(data1.info(),\"\\n\")\n",
        "print('________________________________________\\n')\n",
        "#Categorical features. There is only one categorical feature .i.e 'Country'\n",
        "print('Number  of classes and the value of the most frequent class for categorical features:\\n')\n",
        "print(data1['country'].describe())\n",
        "print('________________________________________\\n')\n",
        "#Numerical features\n",
        "print('Printing statistics for numerical features:\\n')\n",
        "for column in data1:\n",
        "    if(is_numeric_dtype(data1[column])):\n",
        "        print('Mean, Standard deviation, Minimum, Maximum,  for:', column,\"\\n\")\n",
        "        print(np.mean(data1[column]), np.std(data1[column]), data1[column].min(),data1[column].max(),\"\\n\")\n",
        "        print('________________________________________\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngoJnEIO44TZ"
      },
      "source": [
        "(c) For each feature, provide a histogram (with at least 10 bins each) showing the value distribution. Can you spot any obvious data quality issues, e.g. inconsistencies, implausible values or missing values (without researching on specific domain knowledge)?\n",
        "\n",
        "Briefly explain the issues you identified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "2A0b0Wzv44Ta"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "#printing histograms with 15 bins\n",
        "for column in data1:\n",
        "    if(is_numeric_dtype(data1[column])):\n",
        "        sns.displot(data=data1, x=column, bins=15, kde=False);\n",
        "plt.figure(figsize=(20,40))\n",
        "    \n",
        "# Generating box plot\n",
        "fig, axs = plt.subplots(3, 2, figsize=(20,20))\n",
        "data1.plot(y=[\"children_per_woman_total_fertility\"], ax=axs[0, 0], kind=\"box\", title=\"Boxplot for children_per_woman_total_fertility\")\n",
        "data1.plot(y=[\"co2_emissions_tonnes_per_person\"], ax=axs[0, 1], kind=\"box\", title=\"Boxplot for co2_emissions_tonnes_per_person\")\n",
        "data1.plot(y=[\"vccin_effect_dag\"], ax=axs[1, 0], kind=\"box\", title=\"Boxplot for vccin_effect_dag\")\n",
        "data1.plot(y=[\"life_expectancy_years\"], ax=axs[1, 1], kind=\"box\", title=\"Boxplot for life_expectancy_years\")\n",
        "data1.plot(y=[\"corruption_perception_index_cpi\"], ax=axs[2, 0], kind=\"box\", title=\"Boxplot for corruption_perception_index_cpi\")\n",
        "fig.delaxes(axs[2, 1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2UhlbA444Ta"
      },
      "source": [
        "Explanation: As evident in the histogram for children_per_woman_total_fertility, some values are negative. However this is not possible. The histogram for co2_emissions_tonnes_per_person has some gap in the data. This could be the missing data and there can also be outliers. Similarly for vccin_effect_dag  can have some values  missing and there is a possibility of outliers. \n",
        "To have greater confidence, we use box plot to shed light on outliers. \n",
        "The boxplots indicates 'co2_emissions_tonnes_per_person' has outliers in the range of 14 and above. The 'vccin_effect_dag' has an outlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6drQYLF44Tb"
      },
      "source": [
        "(d) Substitute all implausible values as missing data (numpy.nan). Show the scatter matrix of the resulting dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "meK0CqR544Tb"
      },
      "source": [
        "# d) Substituting values which are 0 by NaN\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "print(\"\\n Count of null values in dataset: \\n\",data1.isnull().sum())\n",
        "print('Before making implausible values as NaN')\n",
        "print(data1['children_per_woman_total_fertility'].head())\n",
        "\n",
        "data1[['children_per_woman_total_fertility']] = data1[['children_per_woman_total_fertility']].replace(-10, np.NaN)\n",
        "\n",
        "print('After making implausible values as NaN')\n",
        "print(data1['children_per_woman_total_fertility'].head())\n",
        "       \n",
        "scatter_matrix(data1, alpha=0.5, figsize=(20,20), diagonal='kde')\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb4b9gA544Tc"
      },
      "source": [
        "(e) We need to handle any implausible or missing data. In the lecture, several strategies to do so have been introduced. \n",
        "\n",
        "In this question, consider implausible values to be the ones identified in part (c).\n",
        "\n",
        "    1) For all numerical features, compute and show mean, standard deviation, minimum and maximum, while ignoring the missing and implausible values. Also, print the total number of data rows.\n",
        "    \n",
        "    2) Based on the information obtained in the previous subtasks of this question, choose a strategy for handling all missing/implausible values, such that\n",
        "    - for one feature, you delete all data rows that include a missing value.\n",
        "    - for one feature, you replace all missing values by the median value.\n",
        "    - for one feature, you impute the values based on other, continous features using a regression classifier.\n",
        "    Create a cleaned dataset with all those values handled accordingly. \n",
        "    \n",
        "    3) For all numerical features, compute and show mean, standard deviation, minimum and maximum with respect to your cleaned dataset. Also print the total number of data rows.\n",
        "    \n",
        "    4) Motivate and explain the choices you made in 2). Compare the computed statistical values before and after cleaning and briefly describe and evaluate any changes.\n",
        " \n",
        "*Hint: There might not be an obvious choice for the best strategy. In this case, sound reasoning based on correct observations is more important than the decision itself.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjVAUqeX44Td"
      },
      "source": [
        "# e)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "import pandas as pd\n",
        "\n",
        "print('Total number of data rows before cleaning \\n ',data1.shape[0])\n",
        "print('Printing statistics for numerical features ignoring NaNs:\\n')\n",
        "for column in data1:\n",
        "    if(is_numeric_dtype(data1[column])):\n",
        "        print('Mean, Standard deviation, Minimum, Maximum,  for:', column,\"\\n\")\n",
        "        print(np.nanmean(data1[column]), np.nanstd(data1[column]), np.nanmin(data1[column]),np.nanmax(data1[column]),\"\\n\")\n",
        "        print('________________________________________\\n')\n",
        "print(\"\\n Count of missing values in dataset before cleaning: \\n\",data1.isnull().sum())       \n",
        "print('________________________________________\\n')\n",
        "\n",
        "#replace missing values with mean \n",
        "dsCleaned = data1.copy(deep=True)\n",
        "dsCleaned['vccin_effect_dag'].fillna(value = dsCleaned['vccin_effect_dag'].mean(), inplace=True)\n",
        "\n",
        "#delete rows with missing values\n",
        "dsCleaned.dropna(axis=0, subset=['corruption_perception_index_cpi'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "ds = dsCleaned.copy(deep=True)\n",
        "ds = ds.drop([\"country\"], axis=1)\n",
        "col = list(ds.columns)\n",
        "#Using Imputer\n",
        "imputer = SimpleImputer(strategy = \"median\")\n",
        "transformed = imputer.fit_transform(ds)\n",
        "dsImputed = pd.DataFrame(transformed, columns=col)\n",
        "\n",
        "#Using correlation matrix\n",
        "corMatrix = ds.corr()\n",
        "print ('\\n Using correlaton matrix to find features for correlated to  children_per_woman_total_fertility \\n ',corMatrix['children_per_woman_total_fertility'])\n",
        "print('________________________________________\\n')\n",
        "#Using linear regression\n",
        "x1 = dsImputed[[ 'child_mortality_0_5_year_olds_dying_per_1000_born', 'life_expectancy_years']]\n",
        "y1 = dsImputed['children_per_woman_total_fertility']\n",
        "classifier = LinearRegression()\n",
        "classifier.fit(x1,y1)\n",
        "dsCleaned['children_per_woman_total_fertility'] = y1.values\n",
        "print('\\n Total number of data rows after cleaning \\n ',dsCleaned.shape[0])\n",
        "print('________________________________________\\n')\n",
        "\n",
        "#printing stats\n",
        "print('Printing statistics for numerical features after cleaning:\\n')\n",
        "for column in dsCleaned:\n",
        "    if(is_numeric_dtype(dsCleaned[column])):\n",
        "        print('Mean, Standard deviation, Minimum, Maximum,  for:', column,\"\\n\")\n",
        "        print(np.mean(dsCleaned[column]), np.std(dsCleaned[column]), dsCleaned[column].min(), dsCleaned[column].max(),\"\\n\")\n",
        "        print('________________________________________\\n')\n",
        "        \n",
        "print(\"\\n Count of missing values in dataset after cleaning: \\n\",dsCleaned.isnull().sum())  \n",
        "\n",
        "print('________________________________________\\n')\n",
        "\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "print('\\n Histogram for children_per_woman_total_fertility using cleaned dataset \\n')\n",
        "sns.displot(data=dsCleaned, x='children_per_woman_total_fertility', bins=15, kde=False);\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9bkk6dB44Te"
      },
      "source": [
        "### Explanation: \n",
        "Before handling missing values, there are 3 features with missing values 'vccin_effect_dag', 'corruption_perception_index_cpi' and 'children_per_woman_total_fertility'.\n",
        "The feature 'vccin_effect_dag' has huge number of missing values i.e. around 70. Removing the rows with missing values might lead to loss of important information from other columns. Therefore, we have used mean to fill the missing values for this feature.\n",
        "For the feature 'corruption_perception_index_cpi', we have dropped the missing value as it had only 1 such missing value. So it  resulted in loss of data for 1 row. The total rows after cleaning are 125. This is acceptable.\n",
        "For the feature 'children_per_woman_total_fertility', the implausible values of -10 were replaced by numpy.NaN in Q1d). We imputed missing values using linear regression. Using correlation matrix we found that 'child_mortality_0_5_year_olds_dying_per_1000_born' and 'life_expectancy_years' are stronlgy correlated with 'children_per_woman_total_fertility'.\n",
        "\n",
        "After computing the statistics of cleaned dataset we found:\n",
        "\n",
        "a) The mean and standard deviation for 'children_per_woman_total_fertility' decreased slightly. If we compare the histograms for the feature. The histogram for the feature  in the cleaned data set has more values around the mean as compared to the one we found on 'data1'. This implies that the missing values could have been replaced with values around the mean and it lead to slight decrease.\n",
        "\n",
        "b) The mean and standard deviation for 'child_mortality_0_5_year_olds_dying_per_1000_born' decreased slightly. This slight change can be attributed to reduction in total rows.\n",
        "\n",
        "c) The mean and standard deviation for 'co2_emissions_tonnes_per_person' increased slightly. This slight change can be attributed to reduction in total rows.\n",
        "\n",
        "d) No change in mean and standard deviation for 'corruption_perception_index_cpi' was observed.The missing values had no impact on mean and standard deviation.\n",
        "\n",
        "e) The mean for 'life_expectancy_years' slighlty increased to 72.93 from 72.79 but the standard deviation decreased  to 7.17 from 7.31. This slight change can be attributed to reduction in total rows.\n",
        "\n",
        "f) For 'vccin_effect_dag' the standard deviation decreased to  0.03 from 0.05. Rest remained same. Since we replaced missing value with mean therefore mean remained same in the cleaned dataset but the standard deviation changed becuase number of rows changed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7xS4Kis44Tf"
      },
      "source": [
        "## Question 2 - Data Preprocessing and Advanced Visualization (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUFQmM-q44Tf"
      },
      "source": [
        "For this question, use the <b>dataPrepViz_sampled</b> dataset you created in Q1, part (a)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abpHf6DL44Tg"
      },
      "source": [
        "(a) To create a suitable input for the following questions, modify the dataset as listed below:\n",
        "\n",
        "    1) remove rows that contain negative values\n",
        "    2) remove all rows that contain missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_AxBstA44Th"
      },
      "source": [
        "# your code\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.plotting import parallel_coordinates\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import seaborn as sns\n",
        "\n",
        "# Import dataset\n",
        "dsSampled = pd.read_csv(r\"IDS_Assignment2/DataFiles/dataPrepViz_sampled.csv\",sep=\",\")\n",
        "print(dsSampled.describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rtk_qpeCWHdY"
      },
      "source": [
        "# Remove negative values\n",
        "dsModified = dsSampled[~((dsSampled[['children_per_woman_total_fertility', 'child_mortality_0_5_year_olds_dying_per_1000_born', 'co2_emissions_tonnes_per_person', 'corruption_perception_index_cpi', 'life_expectancy_years','vccin_effect_dag']] < (0))).any(axis=1)]\n",
        "\n",
        "\n",
        "print(\"Sum of Null values in each column:\\n\")\n",
        "print(dsModified.isnull().sum())\n",
        "print(\"\\nSum of 0 value in each column:\\n\")\n",
        "print((dsModified == 0).sum())\n",
        "\n",
        "# Remove rows that contain missing values\n",
        "dsModified.dropna(inplace=True) \n",
        "print(\"\\nShape of modified dataset: \",dsModified.shape)\n",
        "dsModified.to_csv(r\"IDS_Assignment2/DataFiles/dsModified.csv\",header=True,index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjbXtVzSWMmL"
      },
      "source": [
        "### Explanation:<br>\n",
        "Most of the missing values are in *`geographic_group`* and *`vccin_effect_dag`* columns. We use `dropna()` function to remove the rows with missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj_pagaL44Ti"
      },
      "source": [
        "(b) For this subtask remove the feature 'country' from the data. Create four parallel coordinate plots that visualize the relation between the numerical attributes for all geographic groups.\n",
        "\n",
        "    1) For the first parallel coordinate plot, use the values unchanged.\n",
        "    \n",
        "    2-4) For the remaining 3 parallel coordinate plots, first normalize all numerical attributes by mapping them individually to the interval between 0 and 1, that is, apply Min-max normalization. Draw the three plots with different orderings of the features (randomized or chosen by interest)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-X9MdL_44Ti"
      },
      "source": [
        "# your code\n",
        "\n",
        "# Drop column country\n",
        "dsModified_without_country = dsModified.copy(deep=True)\n",
        "dsModified_without_country.drop([\"country\"], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN2sxZR_WRz5"
      },
      "source": [
        "# Plot1\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.title(\"Plot 1\")\n",
        "parallel_coordinates(dsModified_without_country, 'geographic_group', colormap=plt.get_cmap(\"Set1\"))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiMkITN_WT_6"
      },
      "source": [
        "# Applying min-max normalization to numerical features\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(dsModified_without_country[['children_per_woman_total_fertility','child_mortality_0_5_year_olds_dying_per_1000_born','co2_emissions_tonnes_per_person','corruption_perception_index_cpi','life_expectancy_years','vccin_effect_dag']])\n",
        "\n",
        "dsModified_without_country[['children_per_woman_total_fertility','child_mortality_0_5_year_olds_dying_per_1000_born','co2_emissions_tonnes_per_person','corruption_perception_index_cpi','life_expectancy_years','vccin_effect_dag']] = scaler.transform(dsModified_without_country[['children_per_woman_total_fertility','child_mortality_0_5_year_olds_dying_per_1000_born','co2_emissions_tonnes_per_person','corruption_perception_index_cpi','life_expectancy_years','vccin_effect_dag']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rAq4XcwWXa1"
      },
      "source": [
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "# Plot2\n",
        "plt.subplot(4,1,1)\n",
        "plt.title(\"Plot 2\")\n",
        "parallel_coordinates(dsModified_without_country, 'geographic_group', colormap=plt.get_cmap(\"Paired\"))\n",
        "\n",
        "# Plot3\n",
        "plt.subplot(4,1,2)\n",
        "plt.title(\"Plot 3\")\n",
        "parallel_coordinates(dsModified_without_country, 'geographic_group',cols=['corruption_perception_index_cpi','co2_emissions_tonnes_per_person','children_per_woman_total_fertility','vccin_effect_dag','life_expectancy_years','child_mortality_0_5_year_olds_dying_per_1000_born'], colormap=plt.get_cmap(\"Paired\"))\n",
        "\n",
        "# Plot4\n",
        "plt.subplot(4,1,3)\n",
        "plt.title(\"Plot 4\")\n",
        "parallel_coordinates(dsModified_without_country, 'geographic_group',cols=['life_expectancy_years','vccin_effect_dag','children_per_woman_total_fertility','co2_emissions_tonnes_per_person','corruption_perception_index_cpi','child_mortality_0_5_year_olds_dying_per_1000_born'], colormap=plt.get_cmap(\"Paired\"))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAz4NMFaWZ4G"
      },
      "source": [
        "### Explanation: <br>\n",
        "The plots can also be found at `Outputs/Q2b_Plot1` and `Outputs/Q2b_Plots_234`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qINS0Zt244Tj"
      },
      "source": [
        "(c) For each of the questions 1-3 below:\n",
        "- Indicate all of your parallel coordinate plots, which are suitable for finding an answer to the question. Explain your selection. \n",
        "- If possible, briefly answer the questions.\n",
        "\n",
        "    1) Is there a correlation between fertility and CO2 emissions? If yes, is it positive or negative?\n",
        "\n",
        "    2) Is there a correlation between life expectancy and vaccination confidence? If yes, is it positive or negative?\n",
        "\n",
        "    3) Is there a correlation between CO2 emissions and perceived corruption? If yes, is it positive or negative?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joziqBZi44Tk"
      },
      "source": [
        "### Explanation: <br>\n",
        "1) We use Plot 3 and 4 for this part as these plots have the columns together to show the correlation. We see a slight negative correlation between fertility and CO2 emission in both the plots. Some eastern mediterranean and americas regions are also outliers to this.<br>\n",
        "2) All the plots 1,2,3 and 4 have life expectancy and vaccination confidence together. Plot 1 shows a slight positive correlation between the two columns but the data is not enough to verify this as its difficult to check if the plot lines intersect. The normalized values in plots 2,3 and 4 show some geographic_groups intersecting so any correlation is not clear with our data.<br>\n",
        "3) Plot 1 shows a positive correlation between CO2 emissions and percieved corruption. But after values are normalized, plots 2,3 and 4 show no correlation between the two columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdRHETv44Tl"
      },
      "source": [
        "(d) In this subtask we prepare the data for the heat map, which we create in subtask (e). \n",
        "\n",
        "The heat map should visualize the vaccination confidence ('vccin_effect_dag') for different combinations of CO2 emissions ('co2_emissions_tonnes_per_person') and fertility ('children_per_woman_total_fertility'). The heatmap should have 40 columns and 40 rows. The shown vaccination confidence value should be the *median* of all values for each combination of CO2 emissions and fertility. \n",
        "\n",
        "Do the following steps in preparation:\n",
        "\n",
        "    1) First, drop all columns that are not needed in this task.\n",
        "\n",
        "    2) Discretize the CO2 emissions and fertility data into 40 bins each, using equal-width binning.\n",
        "\n",
        "    3) Group the data by CO2 emissions and fertility, using median to aggregate the values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7-jwx2k44Tm"
      },
      "source": [
        "# your code\n",
        "\n",
        "# Drop columns\n",
        "dsHeatMap = dsModified.copy(deep=True)\n",
        "dsHeatMap.drop(['geographic_group','country','life_expectancy_years','corruption_perception_index_cpi','child_mortality_0_5_year_olds_dying_per_1000_born'], axis=1, inplace=True)\n",
        "print(dsHeatMap.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1C4luu_WrLx"
      },
      "source": [
        "# Discretization the CO2 emissions and fertility data with equal width\n",
        "ct = ColumnTransformer(transformers=[(\"equal\", preprocessing.KBinsDiscretizer(n_bins=[40,40], encode='ordinal', strategy = 'uniform'), [0,1])],remainder='passthrough')\n",
        "ct.fit(dsHeatMap)\n",
        "dsHeatMap = ct.transform(dsHeatMap)\n",
        "\n",
        "# Convert to DataFrame\n",
        "dsHeatMap = pd.DataFrame(dsHeatMap,columns=[\"children_per_woman_total_fertility\",\"co2_emissions_tonnes_per_person\",\"vccin_effect_dag\"])\n",
        "print(\"Shape of transformed dataset:\",dsHeatMap.shape)\n",
        "\n",
        "# Grouping CO2 emission and fertility using median to aggregate values\n",
        "print(\"\\nPivot table by grouping CO2 emission and fertility on median:\\n\")\n",
        "dsHeatMap_pivot = pd.pivot_table(dsHeatMap, index=[\"children_per_woman_total_fertility\"],columns=[\"co2_emissions_tonnes_per_person\"] ,values=\"vccin_effect_dag\", aggfunc=np.median)\n",
        "print(dsHeatMap_pivot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO19fX18WulO"
      },
      "source": [
        "### Explanation:<br>\n",
        "`KBinsDiscretizer()` with **uniform** strategy is used to perform equal width binning. We use KBinsDiscretizer() with `ColumnTranformer` so that values of *`vccin_effect_dag`* are not binned with the other columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WodrNMUL44Tn"
      },
      "source": [
        "(e) Use the modified data to create a heat map as specified in part (d). Answer the following questions based on that heat map and briefly explain how you derived your answer:\n",
        "\n",
        "    1) Which combination of bins results in the highest vaccination confidence? \n",
        "    \n",
        "    2) How do you explain empty fields in your heat map?\n",
        "    \n",
        "    3) Can you identify any pattern in the heat map, e.g. in the coloring or in the distribution of empty fields? What can be a possible reason for this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpAdS3mp44To"
      },
      "source": [
        "# your code\n",
        "# Create Heatmap\n",
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "sns.heatmap(dsHeatMap_pivot, annot=True, linewidths=.5, ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z98--ZQk44To"
      },
      "source": [
        "### Explanation: <br>\n",
        "1. Highest vaccination confidence of `0.17` is achieved by below bins:<br>\n",
        "* co2_emissions_tonnes_per_person = 10, children_per_woman_total_fertility = 5\n",
        "* co2_emissions_tonnes_per_person = 14, children_per_woman_total_fertility = 0\n",
        "<br>\n",
        "2. The empty fields in the heat map mean that our dataset does not have the data of *`vccin_effect_dag`* for those combination of bin values of *`co2_emissions_tonnes_per_person`* and *`children_per_woman_total_fertility`*.<br>\n",
        "3. From the heat map we can see that the vaccination confidence is generally high for lower fertility rates but a comparison is still not possible as our dataset does not have data of vaccination confidence value for higher fertility rates for the same bins of CO2 emissions. Vaccination confidence data for combination of high fertility(> 13th bin) and CO2 emission(> 2nd bin) rates is mostly empty in our heat map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H8ZJtK244Tp"
      },
      "source": [
        "## Question 3 - Frequent Item Sets and Association Rules (15 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjsRtbtJ44Tp"
      },
      "source": [
        "(a) Carry out some preprocessing steps before starting the analysis:\n",
        " - Select 90% of the <b>store_data</b> dataset by random sampling. Use the matriculation number of one of the group members as seed.\n",
        " - After completing this preprocessing step, export your final dataset as <b>store_data_2.csv</b> dataset and use it for the next steps of the assignment.\n",
        " - <font color='red'>Important!</font> Make sure that you submit your extracted dataset with your results in Moodle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNFbVJfY44Tq"
      },
      "source": [
        "# your code\n",
        "import pandas as pd\n",
        "import csv\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "\n",
        "df = pd.read_csv('C:/Users/7000024133/Downloads/store_data.csv')\n",
        "\n",
        "new_data_set = []\n",
        "with open(\"C:/Users/7000024133/Downloads/store_data.csv\") as csvFile:\n",
        "    reader = csv.reader(csvFile)\n",
        "    for row in reader:\n",
        "        for item in row:\n",
        "            index = row.index(item)\n",
        "            if item[0]==\"\":\n",
        "                print(item)\n",
        "                print(row)\n",
        "                row[index] =item[1:]\n",
        "                print(row)\n",
        "        new_data_set.append(row)\n",
        "\n",
        "       \n",
        "te = TransactionEncoder()\n",
        "te_array = te.fit(new_data_set).transform(new_data_set)\n",
        "data =pd.DataFrame(te_array, columns = te.columns_)\n",
        "\n",
        "# dataset by random sampling\n",
        "dataset_df = data.sample(frac=0.9, random_state = 414331 )\n",
        "dataset_df.to_csv('C:/Users/7000024133/Downloads/store_data_2.csv', index=False, header=False)\n",
        "dataset_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcP9l63v44Tr"
      },
      "source": [
        "(b) Find the most frequent itemsets with the support of more than 0.04 using the Apriori algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6QYOd9ju44Ts"
      },
      "source": [
        "# your code\n",
        "\n",
        "# Apriori algorithm from mlxtend\n",
        "frequent_itemsets = apriori(dataset_df, min_support = 0.04, use_colnames = True)\n",
        "frequent_itemsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xG2agFW-44Tt"
      },
      "source": [
        "(c) Find the most frequent itemsets with more than 1 member and a support of more than 0.04 using the Apriori algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thM_vP3l44Tt"
      },
      "source": [
        "# your code\n",
        "\n",
        "# Add another column to frequent_itemsets which indicates the number of items in each frequent itemset.\n",
        "frequentdata_itemsets['length'] = frequentdata_itemsets['itemsets'].apply(lambda x: len(x))\n",
        "#frequent_itemsets\n",
        "\n",
        "# Filter frequent_itemsets which have a length higher than 1  \n",
        "frequentdata_itemsets_filtered = frequentdata_itemsets.loc[(frequentdata_itemsets['length'] > 1)]   \n",
        "frequentdata_itemsets_filtered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9XVzev844Tu"
      },
      "source": [
        "(d) Find the itemsets having min_confidence=0.3 and min_lift=1.2. Print support, confidence, and lift of the filtered rules in one table. How do you interpret the quality of the discovered rules?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaTet5P144Tu"
      },
      "source": [
        "# your code\n",
        "from mlxtend.frequent_patterns import association_rules as arule\n",
        "# Association rules\n",
        "association_rules = arule(frequentdata_itemsets,metric ='lift' , min_threshold = 1.2)\n",
        "\n",
        "# Filtering the results which is having minimum confidence 0.3 and minimum lift 1.2\n",
        "filtered_rules = association_rules.loc[(association_rules['lift'] > 1.2) & (association_rules['confidence'] > 0.3)]   \n",
        "\n",
        "# Show the columns 'support', 'confidence' and 'lift' of variable 'filtered_rules' \n",
        "filtered_rules[['support', 'confidence', 'lift']]\n",
        "filtered_rules"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ5nLcgC44Tv"
      },
      "source": [
        "Explanation:Association rule X ==> Y, when filtering the lift to be higher than 1.2 for that we only showed the rows where the items are positively correlated. Support of the product is calculated as the ratio of the number of transactions includes that product and the total number of transactions.Support represents the popularity of that product of all the product transactions.Confidence says how likely an item is purchased when another item is also purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with the first item, in which the other item also appears. when filtering the data by setting the lift to 1,2, support to 0,04 and confidence to 0,3 it yeilds the displayed 4 results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJKW73K444Tv"
      },
      "source": [
        "(e) Apply the FP-Growth algorithm for all the settings of b, c, and d."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOmhbSl-44Tw"
      },
      "source": [
        "# your code\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from mlxtend.frequent_patterns import fpgrowth\n",
        "\n",
        "# FP Growth for b part\n",
        "new_frequent_itemsets=fpgrowth(dataset_df, min_support=0.04, use_colnames=True)\n",
        "print(new_frequent_itemsets)\n",
        "print()\n",
        "# FP Growth for c part\n",
        "new_frequent_itemsets['length'] = new_frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
        "print(new_frequent_itemsets[new_frequent_itemsets['length']>1])\n",
        "print()\n",
        "\n",
        "# FP Growth for d part\n",
        "new_association_rules = arule(new_frequent_itemsets, metric =\"lift\",min_threshold=1.2)\n",
        "new_filtered_rules=new_association_rules[new_association_rules[\"confidence\"]>=0.3]\n",
        "new_filtered_rules[['support', 'confidence', 'lift']]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gjv4iStA44Tx"
      },
      "source": [
        "## Question 4 - Text Mining (15 points): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzCgLTZ944Ty"
      },
      "source": [
        "In this question, we use <b>sms_data.csv</b>. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "zcdI863844Tz"
      },
      "source": [
        "import pandas as pd\n",
        "dsSmSData = pd.read_csv(r\"IDS_Assignment2/DataFiles/sms_data.csv\", sep=';',encoding= 'unicode_escape')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oLE19X44Tz"
      },
      "source": [
        "a) Load the dataset and create the <b>sampled_data</b> dataset which includes 90% of the data. Use the matriculation number of one of the group members as seed. Export the sampled dataset. Split the sampled data into training (80%) and test (20%) data preserving the distribution based on \"Label\".\n",
        "\n",
        "<font color='red'>Important!</font> Make sure that you submit your extracted dataset with your results in Moodle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CE_LJbYK44T0"
      },
      "source": [
        "# Random sampling the data\n",
        "dsSms_Sampled = dsSmSData.sample(frac=0.9, random_state = 414331)\n",
        "\n",
        "# Print data to csv file\n",
        "dsSms_Sampled.to_csv(r\"IDS_Assignment2/DataFiles/sampled_sms_data.csv\",index=False,header=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "corpus_Train, corpus_Test, corpus_train, corpus_test = train_test_split(dsSms_Sampled['Text'],dsSms_Sampled['Label'], test_size=0.2, random_state=414331, stratify=dsSms_Sampled['Label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpvjLDWY44T0"
      },
      "source": [
        "### Classification\n",
        "\n",
        "In the following tasks, train each of the specified models with the training data and give for each the accuracy on the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQcYawO44T1"
      },
      "source": [
        "(b) Model based on the binary document-term matrix\n",
        "\n",
        "Perform preprocessing on the training corpus (all lowercase, no punctuation, tokenization, stemming, and stopword removal) and obtain a binary document-term matrix. Train a logistic classifier with the 'Label' as target feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H0SAWRf44T1"
      },
      "source": [
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords')\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# preprocessing the training corpus\n",
        "def preProcessAndTokenize(corpus):\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    tokenized_words=tokenizer.tokenize(corpus.lower())\n",
        "    filtered_words=[]\n",
        "    for w in tokenized_words:\n",
        "        if w.lower() not in stop_words:\n",
        "            filtered_words.append(w)    \n",
        "    snowball_stemmer = SnowballStemmer('english', ignore_stopwords=True)    \n",
        "    stemmed = [snowball_stemmer.stem(word) for word in filtered_words]\n",
        "    return stemmed\n",
        "\n",
        "\n",
        "vectorizer_binary = CountVectorizer(lowercase=True, preprocessor=None, binary=True, stop_words=None,\n",
        "                                    tokenizer=preProcessAndTokenize)\n",
        "\n",
        "corpus = corpus_Train.values # Text values form training corpus\n",
        "labels = corpus_train.values # Label values form training corpus\n",
        "\n",
        "# Binary document obtained\n",
        "binaryDocumentMatrix = vectorizer_binary.fit_transform(corpus)\n",
        "\n",
        "binaryPipeline = Pipeline([('vect', vectorizer_binary), \n",
        "                            ('logistic', LogisticRegression(C=0.1,multi_class='ovr'))])\n",
        "\n",
        "binaryPipeline = binaryPipeline.fit(corpus, labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY0tu4GX44T2"
      },
      "source": [
        "(c) Model based on doc2vec\n",
        "\n",
        "- Perform preprocessing on the training corpus (all lowercase, no punctuation, tokenization, stemming, and stopword removal). \n",
        "- Obtain a doc2vec embedding in order to reduce the dimension of the document vector. Explain which vector size you use and why.\n",
        "- Use the doc2vec model you just trained to convert the training set to a set of document vectors.\n",
        "- Train a logistic classifier with 'Label' as target feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3rdaWDZ244T2"
      },
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        " \n",
        "tagged_data = [TaggedDocument(words=preProcessAndTokenize(doc), tags=[labels[i]]) for i, doc in enumerate(corpus)]\n",
        "\n",
        "model = Doc2Vec(documents=[documents for documents in tagged_data], vector_size=50, epochs=40,\n",
        "                min_count=2, dm =0)\n",
        "  \n",
        "\n",
        "# Preprocessing training data for doc2vec model\n",
        "preProcessedCorpus = [preProcessAndTokenize(doc) for doc in corpus]\n",
        "\n",
        "# Get vectors of size 50 from trained Doc2Vec model\n",
        "doc2vecTrain = [model.infer_vector(doc) for doc in preProcessedCorpus]\n",
        "\n",
        "# Using logistic regression\n",
        "doc2vecRegressor = LogisticRegression(C=0.1,multi_class='ovr')\n",
        "doc2vecRegressor.fit(doc2vecTrain, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXtsmRLl44T2"
      },
      "source": [
        "### Evaluation\n",
        "For the following tasks, use the test data.\n",
        "\n",
        "(d) Predict the classification with the two models on the test data. Preprocess the data if necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JG4DRLkH44T3"
      },
      "source": [
        "import numpy as np\n",
        "corpusTest = corpus_Test.values # Text values form test corpus\n",
        "labelsTest = corpus_test.values # Label values form test corpus\n",
        "\n",
        "\n",
        "# Preprocess corpus\n",
        "corpusTestDoc2Vec = [preProcessAndTokenize(doc) for doc in corpusTest]\n",
        "\n",
        "# Obtain vectors using doc2vec model\n",
        "doc2vecTest = [model.infer_vector(doc) for doc in corpusTestDoc2Vec]\n",
        "\n",
        "prediction_doc2vec = doc2vecRegressor.predict(doc2vecTest)\n",
        "print('\\nPrediction accuracy on test set using Doc2Vec :',np.mean(prediction_doc2vec == labelsTest))\n",
        "\n",
        "print('____________________________________')\n",
        "prediction_binary = binaryPipeline.predict(corpusTest)\n",
        "print('\\nPrediction accuracy on test set using Binary document-term matrix:', np.mean(prediction_binary == labelsTest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSHLvedx44T3"
      },
      "source": [
        "Explanation: The 'vector_size' represents number of dimensions of the feature vector. More dimensions mean more representations. However increasing the vector_size does not mean better representation. Higher value may lead to increase in traning time and increase in memory required to store vectors.\n",
        "We got accuracy as 98.80% for all values when the 'vector_size' was chosen from the set {50,100,300}. We chose 50 as the vector_size because it gives the same accuracy as others with the additional advantage of lesser traning time and memory required to store it. In our opinion higher dimensional vectors are not needed in this case and they may risk overfitting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v45nKNhY44T4"
      },
      "source": [
        "(e) Obtain the confusion matrices for the two models and the prediction on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLJDXPOx44T4"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print('Confusion matrix for model based on the binary document-term matrix\\n ')\n",
        "cm_binaryDocument = metrics.confusion_matrix(labelsTest, prediction_binary)\n",
        "print(cm_binaryDocument)\n",
        "print('____________________________________')\n",
        "print('Confusion matrix for model based on doc2vec\\n ')\n",
        "cm_binaryDocument = metrics.confusion_matrix(labelsTest, prediction_doc2vec)\n",
        "print(cm_binaryDocument)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UTyBg8D44T5"
      },
      "source": [
        "(f) Obtain accuracy and F1-score for the prediction of the two different models on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apKiwy4x44T5"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print('Printing accuracy and F1-score')\n",
        "print('Model name   Accuracy score  F1 score')\n",
        "print('Binary: \\t {:.4f} \\t {:.4f}'.format(accuracy_score(labelsTest, prediction_binary),\n",
        "                                           f1_score(labelsTest, prediction_binary,pos_label='ham')))\n",
        "\n",
        "print('Doc2Vec: \\t {:.4f} \\t {:.4f}'.format(accuracy_score(labelsTest, prediction_doc2vec),\n",
        "                                            f1_score(labelsTest, prediction_doc2vec,pos_label='ham')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orZqEI7z44T6"
      },
      "source": [
        "(g) Briefly comment on the quality of the two models. Interpret the results retrieved in the evaluation part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StsUK4pk44T6"
      },
      "source": [
        "Explanation: Both the models achieve simialr accuracy and high F1-score. This implies that the corpus was easy to classify\n",
        "However the Doc2Vec obtained better F1-score and accuracy as compared to model based on the binary document-term matrix.\n",
        "The confusion matrix shows that  Doc2Vec has more true positives and lesser false negative as compared to model based on the binary document-term matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD9x_b3_44T7"
      },
      "source": [
        "### Language model\n",
        "\n",
        "For the following tasks use the <b>sampled_data</b>.\n",
        "\n",
        "(h) Create two lists, one for ham and one for spam, containing all messages.\n",
        "For ham and spam separately, build a bigram language model using the initial dataset (before splitting to training and test data). Do not perform stemming nor stopword removal for this task, but apply other preprocessing steps, such as all to lowercase, no punctuation and tokenization. Use both right and left padding, and manage unknown terms by using a dedicated token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYCJGJ1c44T7"
      },
      "source": [
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "corpusHam = dsSms_Sampled[dsSms_Sampled['Label'] == 'ham']\n",
        "corpusSpam = dsSms_Sampled[dsSms_Sampled['Label'] == 'spam']\n",
        "\n",
        "#preProcess -all to lowercase, no punctuation and tokenization\n",
        "def preProcess(corpus):\n",
        "    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "    tokenized_words=tokenizer.tokenize(corpus.lower())\n",
        "    return tokenized_words\n",
        "   \n",
        "\n",
        "ham = []\n",
        "for doc in corpusHam.iloc[:, 1]:\n",
        "    ham.append(preProcess(doc))\n",
        "spam = []\n",
        "for doc in corpusSpam.iloc[:, 1]:\n",
        "    spam.append(preProcess(doc))  \n",
        "    \n",
        "#bigram model, setting n = 2\n",
        "n=2\n",
        "#Items that are not seen are mapped to <UNK> by default.\n",
        "trainHam, padded_Ham = padded_everygram_pipeline(n, ham) \n",
        "trainSpam, padded_Spam = padded_everygram_pipeline(n, spam)\n",
        "\n",
        "modelHam = MLE(n)\n",
        "modelSpam = MLE(n)\n",
        "\n",
        "# training the MLEs\n",
        "modelHam.fit(trainHam, padded_Ham)\n",
        "modelSpam.fit(trainSpam, padded_Spam)\n",
        "\n",
        "print(' MLE model for Ham\\'s vocabulary: ',modelHam.vocab, '\\n')\n",
        "print(' MLE model for Spam\\'s vocabulary: ',modelSpam.vocab, '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-51oBwW44T7"
      },
      "source": [
        "(i) For each message groups, use the correspondent language model from (h) to generate, using MLE, a sentence of fifteen words using the following terms as seed:\n",
        "- 'hello'\n",
        "- 'yes'\n",
        "- 'but'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwwVqF4k44T8"
      },
      "source": [
        "print(' 3 sentences of 15 words generated by Ham bigram model  using seeds \"hello\", \"yes\" and \"but\":')\n",
        "print( ' '.join([str(word) for word in modelHam.generate(15, text_seed='hello', random_seed=414331)]))\n",
        "print(' '.join([str(word)  for word in modelHam.generate(15, text_seed='yes',  random_seed=414331 )]))\n",
        "print(' '.join([str(word) for word in modelHam.generate(15,  text_seed='but',random_seed=414331)]))\n",
        "print('________________________________________________________________________________________________________')\n",
        "print('\\n')\n",
        "print(' 3  sentences of 15 words generated by Spam bigram model  using seeds \"hello\", \"yes\" and \"but\":')\n",
        "print(' '.join([str(word) for word in modelSpam.generate(15, text_seed='hello', random_seed=414331)]))\n",
        "print(' '.join([str(word)  for word in modelSpam.generate(15,  text_seed='yes', random_seed=414331)]))\n",
        "print(' '.join([str(word) for word in modelSpam.generate(15, text_seed='but', random_seed=414331)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRiSJ47N44T8"
      },
      "source": [
        "(j) Build a trigram model with the same data as in the previous task. Use both right and left padding, and manage unknown terms by using a dedicated token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jt4T6Au44T9"
      },
      "source": [
        "#trigram model, setting n = 3\n",
        "n=3\n",
        "trainHam3, padded_Ham3 = padded_everygram_pipeline(n, ham)\n",
        "trainSpam3, padded_Spam3 = padded_everygram_pipeline(n, spam)\n",
        "\n",
        "modelHamTrigram = MLE(n)\n",
        "modelSpamTrigram = MLE(n)\n",
        "\n",
        "# training the MLEs\n",
        "modelHamTrigram.fit(trainHam3, padded_Ham3)\n",
        "modelSpamTrigram.fit(trainSpam3, padded_Spam3)\n",
        "\n",
        "print(' MLE trigram model for Ham \\'s vocabulary: ',modelHamTrigram.vocab, '\\n')\n",
        "print(' MLE trigram model for Spam \\'s vocabulary: ',modelSpamTrigram.vocab, '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnWQYarT44T9"
      },
      "source": [
        "(k) For each message group (ham and spam), use the correspondent language model from the previous qustion to generate, using MLE, a sentence of fifteen words using the following terms as seed:\n",
        "- 'hello'\n",
        "- 'yes'\n",
        "- 'but'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiwTIbzQ44T-"
      },
      "source": [
        "print(' 3 sentences of 15 words generated by Ham trigram model  using seeds \"hello\", \"yes\" and \"but\":')\n",
        "print( ' '.join([str(word) for word in modelHamTrigram.generate(15, text_seed='hello', random_seed=414331)]))\n",
        "print(' '.join([str(word)  for word in modelHamTrigram.generate(15, text_seed='yes',  random_seed=414331)]))\n",
        "print(' '.join([str(word) for word in modelHamTrigram.generate(15,  text_seed='but',random_seed=414331)]))\n",
        "print('________________________________________________________________________________________________________')\n",
        "print('\\n')\n",
        "print(' 3 sentences of 15 words generated by Spam trigram model  using seeds \"hello\", \"yes\" and \"but\":')\n",
        "print(' '.join([str(word) for word in modelSpamTrigram.generate(15, text_seed='hello', random_seed=414331 )]))\n",
        "print(' '.join([str(word)  for word in modelSpamTrigram.generate(15,  text_seed='yes', random_seed=414331)]))\n",
        "print(' '.join([str(word) for word in modelSpamTrigram.generate(15, text_seed='but', random_seed=414331)]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0M4iOyv44T-"
      },
      "source": [
        "(l) Compare the quality of the generated text. Which model performs better? In general, which differences are there in using trigrams as opposed to bigrams?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WktTmPV144T-"
      },
      "source": [
        "###Explanation: \n",
        "Trigram model has more contextual data than bigram model. Therefore, it performs better as it takes into account tuples of both length 2 & 3. As evident from the sentences generated for each message group, the trigram model provides better text. \n",
        "Another difference is that it is less expensive to train bigram model than to train a trigram model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bNF7yS44T_"
      },
      "source": [
        "## Question 5 - Process Mining (15 points): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEfUh7uQ44T_"
      },
      "source": [
        "### General investigation\n",
        "\n",
        "a) Import the event log from the <b>Quarantine_Log</b> csv file. Set the case ID to 'patient', Timestamp to 'timestamp' and Activity as 'activity'. Also, set the lifecyle column to the right attribute. Furthermore, identify the case attributes and set them to case attributes. Find the correct setting, so that the resource is understood as resource (compare with the documentation). Give some basic information:\n",
        "\n",
        "    - number of cases\n",
        "    - number of variants\n",
        "    - number of events\n",
        "    - the trace and event attribute names\n",
        "    - the number of resources\n",
        "    - the earliest timestamp and the latest timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEgR2Ji144UA"
      },
      "source": [
        "import pandas as pd\n",
        "import pm4py\n",
        "from pm4py.objects.conversion.log import converter as log_converter\n",
        "from pm4py.objects.log.util import dataframe_utils\n",
        "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
        "from pm4py.algo.filtering.log.variants import variants_filter\n",
        "from pm4py.objects.log.util import interval_lifecycle\n",
        "\n",
        "\n",
        "log_csv = pd.read_csv('C:/Users/7000024133/Downloads/Quarantine_log.csv', sep=',')\n",
        "log_csv.sort_values('Timestamp',kind='mergesort')  \n",
        "log_csv.rename(columns={'Patient': 'case:concept:name'}, inplace=True)\n",
        "log_csv.rename(columns={'Activity': 'concept:name'}, inplace=True)\n",
        "log_csv.rename(columns={'Timestamp': 'time:timestamp'}, inplace=True)\n",
        "log_csv.rename(columns={'Lifecycle': 'lifecycle:transition'}, inplace=True)\n",
        "log_csv.rename(columns={'Resource': 'org:Resource'}, inplace=True)\n",
        "log_csv.rename(columns={'PatientName': 'case:patientname'}, inplace=True)\n",
        "log_csv.rename(columns={'Insurance': 'case:insurance'}, inplace=True) \n",
        "log_csv.rename(columns={'Age': 'case:age'}, inplace=True)\n",
        "log_csv.rename(columns={'Type': 'concept:type'}, inplace=True)\n",
        "\n",
        "log_csv = dataframe_utils.convert_timestamp_columns_in_df(log_csv)\n",
        "log_with_cloud =log_csv[log_csv[\"concept:type\"]=='cloud']\n",
        "\n",
        "parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'}\n",
        "event_log = log_converter.apply(log_csv, parameters=parameters)\n",
        "event_log = interval_lifecycle.to_interval(event_log)\n",
        "Number_of_Cases = log_csv['case:concept:name'].nunique()\n",
        "Number_of_resources = log_csv['org:Resource'].nunique()\n",
        "number_of_Events = log_csv['case:concept:name'].nunique()\n",
        "earliest_timestamp = log_csv['time:timestamp'].iloc[0]\n",
        "latest_timestamp = log_csv['time:timestamp'].iloc[-1]\n",
        "variants = pm4py.get_variants(event_log)\n",
        "trace_attributes = pm4py.get_trace_attributes(event_log)\n",
        "event_attributes = pm4py.get_attributes(event_log)\n",
        "\n",
        "print(\"Number of Cases :\",Number_of_Cases)\n",
        "print(\"Number of Resources :\",Number_of_resources)\n",
        "print(\"Number of Events :\",number_of_Events)\n",
        "print(\"Earlier time Stamp :\",earliest_timestamp)\n",
        "print(\"Latest Time Stamp:\", latest_timestamp) \n",
        "print(\"Trace Attributes  :\",trace_attributes)\n",
        "print(\"Number of Variants :\",len(variants.keys()))\n",
        "print(\"Event Attributes\",event_attributes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMQP58VB44UB"
      },
      "source": [
        "### Sampling the event log\n",
        "(b) Create a sample of the event log (<b>log_sampled</b>) containing 80% of the traces. Export the sampled event log.\n",
        "\n",
        "<font color='red'>Important!</font> Make sure that you submit your extracted event log with your results in Moodle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uFHGlBL44UB"
      },
      "source": [
        "# your code\n",
        "\n",
        "from pm4py.algo.filtering.log.variants import variants_filter\n",
        "from pm4py.objects.conversion.log import converter as log_converter\n",
        "\n",
        "trace_80 = int(1500*0.8)\n",
        "\n",
        "log_sampled=pm4py.objects.log.util.sampling.sample(event_log,trace_80)\n",
        "\n",
        "dataframe = log_converter.apply(event_log, variant=log_converter.Variants.TO_DATA_FRAME)\n",
        "dataframe.to_csv('C:/Users/7000024133/Downloads/log_sampled.csv')\n",
        "dataframe\n",
        "                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTyen5Eq44UC"
      },
      "source": [
        "### Trace frequency\n",
        "(c) Use the sampled event log and print the least frequent and the most frequent variant and the corresponding counts. Is there already some indication about the model structure (e.g. loops, parallel, etc.)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt6lX24_44UD"
      },
      "source": [
        "# your code\n",
        "\n",
        "variants = pm4py.get_variants(log_sampled)\n",
        "totalcases = {}\n",
        "for i in range(len(variants)):\n",
        "    x= list(variants.keys())[i]\n",
        "    totalcases[x]=len(variants[x])\n",
        "    \n",
        "TotalfrequencyList=list(totalcases.values())\n",
        "TotalfrequencyList.sort(reverse=True)\n",
        "\n",
        "leastfrequentvariants = [j for j,k in totalcases.items() if k==TotalfrequencyList[-1]]\n",
        "Mostfrequentvariants = [j for j,k in totalcases.items() if k==TotalfrequencyList[0]]\n",
        "\n",
        "print(\"Least Frequent Variants : \",leastfrequentvariants[-1])\n",
        "print(leastfrequentvariants)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaIO-0cqR-r7"
      },
      "source": [
        "print(\"Most Frequent Variants : \",Mostfrequentvariants[0])\n",
        "print(Mostfrequentvariants)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI4oTB0L44UD"
      },
      "source": [
        "Explanation: In most frequent variants , we are having ['Register,Initial Exam,Initial Exam Decision,Discharge Init Exam'] cases which we got above and commonaly used but in least frequest frequent,there are lot of small variants which we recieved.Some of them creating loops i.e [Treatment A1,Check Treatment A1] ,[Treatment A2,Check Treatment A2,Treatment A1,Check Treatment A1]. I didnt see any variants in parallel for least frequent variants. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aER_QwC544UE"
      },
      "source": [
        "### Filter event logs\n",
        "(d) Create three different event logs:\n",
        "\n",
        "1. One event log containing only the 10% of the most frequent traces (**filtered_log_variants**).\n",
        "2. One event log containing only patients with private insurance (**filtered_log_insurance**).\n",
        "3. One event log containing only patients having the event attribute type as 'cloud' (**filtered_log_cloud**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaIDdmfr44UE"
      },
      "source": [
        "# your code\n",
        "\n",
        "from pm4py.algo.filtering.log.attributes import attributes_filter\n",
        "from pm4py.objects.log.util import dataframe_utils\n",
        "\n",
        "filtered_log_variants = variants_filter.filter_log_variants_percentage(log_sampled, percentage=0.1)\n",
        "print(\"10% of Most Frequent Traces\",len(filtered_log_variants))\n",
        "\n",
        "filtered_log_insurance = attributes_filter.apply_events(log_sampled, [\"PRIV\"],parameters={attributes_filter.Parameters.ATTRIBUTE_KEY: \"case:insurance\", attributes_filter.Parameters.POSITIVE: True})\n",
        "print(\"Events with Private Insurance\",len(filtered_log_insurance))\n",
        "\n",
        "patientwithtypecloud = log_with_cloud[\"case:concept:name\"].tolist()\n",
        "log_with_cloud =log_csv[log_csv[\"case:concept:name\"].isin(patientwithtypecloud)]\n",
        "event_log_cloud = log_converter.apply(log_with_cloud, parameters=parameters,variant=log_converter.Variants.TO_EVENT_LOG)\n",
        "event_log_cloud = interval_lifecycle.to_interval(event_log_cloud)\n",
        "filtered_log_cloud =event_log_cloud\n",
        "print(\"Filterd log Cloud :\",len(filtered_log_cloud))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMfjkool44UF"
      },
      "source": [
        "### Discovery and conformance checking\n",
        "\n",
        "(e) Use the Inductive Miner to discover a process model (Process tree or Petri net) for each event log created in (d). For one of the models - you may choose - explain shortly the behaviour of the model. (e.g. loops, sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x8qUDfj44UF"
      },
      "source": [
        "# your code\n",
        "\n",
        "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
        "\n",
        "tree_variants = inductive_miner.apply_tree(filtered_log_variants)\n",
        "tree_insurance = inductive_miner.apply_tree(filtered_log_insurance)\n",
        "tree_with_cloud = inductive_miner.apply_tree(filtered_log_cloud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlnobPJgkXSw"
      },
      "source": [
        "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
        "\n",
        "gviz = pt_visualizer.apply(tree_variants,parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"})\n",
        "pt_visualizer.view(gviz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y63JyDJHkgfw"
      },
      "source": [
        "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
        "\n",
        "gviz = pt_visualizer.apply(tree_insurance,parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"})\n",
        "pt_visualizer.view(gviz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZipjuoukjIZ"
      },
      "source": [
        "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
        "\n",
        "gviz = pt_visualizer.apply(tree_with_cloud,parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"})\n",
        "pt_visualizer.view(gviz)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EF-dO_v44UF"
      },
      "source": [
        "Explanation:We are going to explain about tree variants process tree. it is underfitted as only two-three traces are used in that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb86ztaJ44UG"
      },
      "source": [
        "(f) Briefly summarize the differences and similarities of the models. Why do they differ/are similar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8-FaeBd44UG"
      },
      "source": [
        "Explanation: \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJKtHLqp44UH"
      },
      "source": [
        "(g) Perform the token-based replay for conformance checking using your discovered model for **filtered_log_variants** and the original event log. Does your process model fit the log? Explain the result in one sentence. Calculate the trace and log fitness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rgOM3rb44UH"
      },
      "source": [
        "# your code\n",
        "from pm4py.visualization.petrinet import visualizer as pn_visualizer\n",
        "from pm4py.objects.conversion.process_tree import converter as pt_converter\n",
        "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
        "from pm4py.algo.conformance.tokenreplay import factory as token_replay_factory\n",
        "\n",
        "print('Trace- ',variants_filter.get_variants([event_log[1200]]).keys())\n",
        "token_replay_result=token_replay_factory.apply(event_log, net, im, fm)\n",
        "print(token_replay_result[1200])\n",
        "replayed_traces = token_replay.apply(event_log, net, im, fm)\n",
        "\n",
        "net, im, fm = pt_converter.apply(tree_variants)\n",
        "gviz =pn_visualizer.apply(net, im, fm)\n",
        "pn_visualizer.view(gviz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JC0GvvxkrMw"
      },
      "source": [
        "from pm4py.evaluation.replay_fitness import evaluator as replay_fitness\n",
        "\n",
        "tokenbased_replay = replay_fitness.apply(event_log,net,im,fm)\n",
        "print(tokenbased_replay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdxmrSyf44UI"
      },
      "source": [
        "Explanation: The model has a low fitness, because of the low number of traces used to create the model but the fitness which we have recieved is calculated based on entire log.Traces that are being used to create the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtfgbgA744UI"
      },
      "source": [
        "### Frequency and performance\n",
        "\n",
        "(h) Visualize the model for the **filtered_log_variants** event log enriched with frequency information. Subsequently, visualize that same model enriched with performance information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aclx5W2W44UJ"
      },
      "source": [
        "# your code\n",
        "from pm4py.visualization.petrinet import visualizer as pn_visualizer\n",
        "\n",
        "gviz = pn_visualizer.apply(net,im,fm,parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"},variant=pn_visualizer.Variants.FREQUENCY,log=filtered_log_variants)\n",
        "pn_visualizer.view(gviz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGNiibzdk_sx"
      },
      "source": [
        "gviz = pn_visualizer.apply(net,im,fm,parameters={pt_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"},variant=pn_visualizer.Variants.PERFORMANCE,log=filtered_log_variants)\n",
        "pn_visualizer.view(gviz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXX1usaF44UJ"
      },
      "source": [
        "(i) What are frequent activities? Why may they be frequent (think about the real life process described by the log)? What are possibly problematic activities according to the performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APR9zH7Abhwj"
      },
      "source": [
        "Explanation: Most frequents activities are Register,Intial Exam,Initial Exam Decision,Discharge Init Exam.Usually we went to hospital and after registration,we do Intial checkup and get the intial exam decision from the doctors. Once doctor provide the suggestion about the result, we get discharge init exam . in emergency cases only, we do lot of test and bed isolation in  hospital and do discharge test. \n",
        "\n",
        "Performace - There are couple of problematic activities which we can see in model i.e after registration ,16 minutes are taken for initial exam which is one of the problematic. We need to work on priority for initial exam after registration. After initial exam decision , they are taking around 1 hour to discharge the patients which is consuming lot of time of customer. One of the problematic is also related to control call. informing about isolation, they are taking 5 hours for control call and 2 hours for doing test III decision.   these all things needs to be improved. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9uFvj2844UK"
      },
      "source": [
        "## Question 6 - Big Data (15 points): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT5q2K6D44UK"
      },
      "source": [
        "### Preparation: generating a simple log\n",
        "\n",
        "In this question, we use the event log from the log csv file with the following modifications:\n",
        "1. We flatten the lifecycles (i.e., start and complete) into a single event. Each event contains the start timestamp and complete timestamp.\n",
        "2. A new column, called ServiceTime column, is included which represents the duration of the corresponding activity in the event.\n",
        "\n",
        "We name the event log as **simple_log** in the remainder. Please follow the explanations below to prepare the **simple_log**. The preparation steps will not be graded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtancVWg44UL"
      },
      "source": [
        "# We use following utility functions for the modifications (these are given):\n",
        "def _distinguish_duplicate_activities(log):\n",
        "    \"\"\"Add flags to the duplicate activities in a trace in order to distinguish them\n",
        "\n",
        "    Keyword arguments:\n",
        "    log -- even log\n",
        "    \"\"\"\n",
        "    trace = list()\n",
        "    activity_list = list()\n",
        "    count=0\n",
        "    prev_caseid=\"\"\n",
        "    for row in log.itertuples():\n",
        "        activity=row.Activity\n",
        "        caseid=row.Patient\n",
        "        if(caseid!=prev_caseid):\n",
        "            count=0\n",
        "            prev_caseid=caseid\n",
        "            trace=[]\n",
        "\n",
        "        if activity in trace:\n",
        "            count+=1\n",
        "            activity = activity + \"-{}\".format(count)\n",
        "            \n",
        "        trace.append(activity)\n",
        "        activity_list.append(activity)\n",
        "    log[\"Activity\"] = activity_list\n",
        "    return log\n",
        "\n",
        "def _merge_lifecylces(log):\n",
        "    \"\"\"Merge lifycycles (start,complete) into a single event. \n",
        "\n",
        "    Keyword arguments:\n",
        "    log -- even log\n",
        "    \"\"\"\n",
        "    start_log = log.loc[log[\"Lifecycle\"]==\"start\"]\n",
        "    start_log = _distinguish_duplicate_activities(start_log)\n",
        "    \n",
        "    complete_log = log.loc[log[\"Lifecycle\"]==\"complete\"]\n",
        "    complete_log = _distinguish_duplicate_activities(complete_log)\n",
        "\n",
        "    complete_log[\"CompleteTime\"] = complete_log[\"ModelTime\"]\n",
        "    simple_log = start_log.merge(complete_log, left_on=['Patient',\"Activity\"], right_on=['Patient',\"Activity\"],suffixes=(\"\", \"_y\"))\n",
        "    simple_log.drop(simple_log.filter(regex='_y$').columns.tolist(),axis=1, inplace=True)\n",
        "    simple_log[\"ServiceTime\"] = simple_log[\"CompleteTime\"] - simple_log[\"ModelTime\"]\n",
        "    return simple_log\n",
        "\n",
        "def produce_simple_log(filepath):\n",
        "    \"\"\"Produce simple log where the lifecycles are merged and service time information is added\n",
        "\n",
        "    Keyword arguments:\n",
        "    filepath -- path to event log\n",
        "    \"\"\"\n",
        "    log = pd.read_csv(filepath, sep=',')\n",
        "    log.sort_values(by=[\"Patient\",\"ModelTime\"],inplace=True)\n",
        "    simple_log = _merge_lifecylces(log)\n",
        "    return simple_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0d3D9u344UL"
      },
      "source": [
        "**Preparation step 1**: Replace the filepath to your own filepath to produce the **simple_log**.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyfAB91944UM"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "#your filepath\n",
        "filepath = \"IDS_Assignment2/DataFiles/Quarantine_log.csv\"\n",
        "simple_log = produce_simple_log(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG50FCE944UM"
      },
      "source": [
        "### Preparation: expanding the event log\n",
        "\n",
        "In this question, we generate 100 event logs based on the <b>simple_log</b>. Each log replicates the base log (i.e., the <b>simple_log</b>). For randomization, you need to use the sum of the group's matriculation numbers (e.g., a group with 3 students having \"100000\", \"100001\", and \"100002\" as their matriculation numbers will use \"300003\" for the randomization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvCogD9p44UN"
      },
      "source": [
        "# We use following utility functions for the modifications (these are given):\n",
        "import random\n",
        "def _randomize_case_attribute(log,matriculation_num):\n",
        "    \"\"\"Randomize case attributes based on the matriculation number\n",
        "\n",
        "    Keyword arguments:\n",
        "    log -- event log\n",
        "    matriculation_num - sum of matriculation numbers\n",
        "    \"\"\"\n",
        "    random.seed(matriculation_num)\n",
        "    caseids = set(log[\"Patient\"])\n",
        "    for caseid in caseids:\n",
        "        random_val = random.randint(-3,3)\n",
        "        random.seed(random_val)\n",
        "        log.loc[log[\"Patient\"]==caseid,\"Age\"] = log.loc[log[\"Patient\"]==caseid,\"Age\"]+random_val\n",
        "    return log\n",
        "\n",
        "def _extract_log(log,iter_num):\n",
        "    \"\"\"Extract n-th log to ./generated_logs/\n",
        "\n",
        "    Keyword arguments:\n",
        "    log -- event log\n",
        "    iter_num -- n-th iteration\n",
        "    \"\"\"\n",
        "    log.to_csv(\"./generated_logs/generated_log-{}.tsv\".format(iter_num),header=False,index=False, sep=\"\\t\",line_terminator=\"\")\n",
        "\n",
        "def generate_log(original_log,num_replication,mat_num):\n",
        "    \"\"\"Generate logs (randomized by the matriculation number and extracted to ./generated_logs/) \n",
        "\n",
        "    Keyword arguments:\n",
        "    log -- event log\n",
        "    num_replication -- number of generated logs\n",
        "    mat_num -- sum of matriculation numbers\n",
        "    \"\"\"\n",
        "    import os\n",
        "    dir_path = \"./generated_logs\"\n",
        "    try:\n",
        "        os.mkdir(dir_path)\n",
        "    except OSError:\n",
        "        print (\"Directory already exists: %s\" % dir_path)\n",
        "    else:\n",
        "        print (\"Successfully created the directory %s \" % dir_path)\n",
        "    \n",
        "    base_log = original_log.copy(deep=True)\n",
        "    max_modeltime = max(base_log[\"ModelTime\"])\n",
        "    max_patientid = max(base_log[\"Patient\"])\n",
        "    for i in range(num_replication):\n",
        "        generated_log = base_log\n",
        "        generated_log[\"Patient\"] += max_patientid\n",
        "        generated_log[\"ModelTime\"] += max_modeltime\n",
        "        random.seed(None)\n",
        "        randomized_log = _randomize_case_attribute(generated_log,random.randint(0,mat_num))\n",
        "        _extract_log(randomized_log,i)\n",
        "        print (\"Successfully created %i th log at %s \"% (i,dir_path))\n",
        "        base_log = randomized_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsLeK7tw44UN"
      },
      "source": [
        "**Preparation step 2**: Generate 100 replicated logs in your local disk (./generated-logs/generated-log-0.tsv, ./generated-logs/generated-log-1.tsv, ..., ./generated-logs/generated-log-99.tsv). Do not forget to replace the SUM_MAT_NUM to yours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRIUdJvn44UO"
      },
      "source": [
        "#your group's sum\n",
        "SUM_MAT_NUM = 1234038 \n",
        "base_log = simple_log[[\"Patient\", \"ModelTime\",\"Activity\",\"Age\",\"ServiceTime\"]] # this will be removed\n",
        "NUM_REPITITION=100\n",
        "generate_log(base_log,NUM_REPITITION,SUM_MAT_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM0vglG3nkV7"
      },
      "source": [
        "### Explanation:<br> \n",
        "Our groups matriculation number sum is 1234038"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN4kwrOc44UO"
      },
      "source": [
        "### Basic Hadoop\n",
        "\n",
        "(a) Now, it's time to work with the Hadoop system. The goal of this task is to merge 100 event logs at your disk in the Hadoop system. Follow the instructions below and show your results in each step (screenshots of the command line). We use \"letter identifier\" for this task (The letter identifier is the string consisting of the first letters of the group memebers' first names, e.g., for the group with \"Alessandro Berti\", \"Bernardo Silva\", \"Chiao Li\", the indentifier is \"ABC\").\n",
        "\n",
        "    1) Import the event logs to your Docker engine (at /usr/local/hadoop/(identifier)-generated-logs/).\n",
        "    2) Upload the files to the running Hadoop system (at /input/(identifier)-generated-logs/). \n",
        "    3) Merge the file and copy the result back to the Hadoop system (at /input/(identifier)-final-log.tsv).\n",
        "    4) Using the Hadoop command, print out the merged file in the command line (the screenshot may contain 10 rows)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAPikKdF44UP"
      },
      "source": [
        "#your code\n",
        "from IPython.display import Image\n",
        "Image(filename='IDS_Assignment2/BigData_Files/Q6a_1.jpg') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1uhcLV744UP"
      },
      "source": [
        "#your code\n",
        "Image(filename='IDS_Assignment2/BigData_Files/Q6a_2.jpg') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdHCGmAP44UQ"
      },
      "source": [
        "#your code\n",
        "Image(filename='IDS_Assignment2/BigData_Files/Q6a_3.jpg') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umH7UHuj44UQ"
      },
      "source": [
        "#your code\n",
        "Image(filename='IDS_Assignment2/BigData_Files/Q6a_4.jpg') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU9E8P26XawR"
      },
      "source": [
        "### Explanation:<br>\n",
        "* The identifier for our group is `KNS` (Kshitij,Neeraj,Sparsh).\n",
        "* The screenshots can also be found in **`BigData_Files/`** folder.\n",
        "* The merged file is at **`BigData_Files\\output\\KNS-final-log.tsv`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca949FgY44UR"
      },
      "source": [
        "### Process Discovery\n",
        "\n",
        "(b) Discover a process model from the merged file using MapReduce algorithms. Explain how you discover the process model with the following deliverables:\n",
        "\n",
        "    1) Mapper function (as python file(s))\n",
        "    2) Reducer function (as python file(s))\n",
        "    3) Hadoop commands for MapReduce calculation (as text file)\n",
        "    4) Jupyter notebook code that prints the directly-follows relations and discover process models based on the directly-follows relations (you are free to use any discovery algorithms)\n",
        "\n",
        "<font color='red'>Important!</font> Please note that in this task, your result will be evaluated based on whether they are reproducible from your explanation. If you skip MapReduce calculations for this task, you will get 0 points.The deliverables of 1), 2), and 3) should be submitted as outputs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4SPgRCe44US"
      },
      "source": [
        "# your code for (b)-4\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import csv\n",
        "from pm4py.objects.log.importer.xes import factory as xes_importer\n",
        "from pm4py.objects.conversion.dfg import factory as dfg_mining_factory\n",
        "from pm4py.algo.discovery.dfg import factory as dfg_factory\n",
        "from pm4py.visualization.dfg import factory as dfg_vis_factory\n",
        "from pm4py.visualization.petrinet import factory as pn_vis_factory\n",
        "\n",
        "\n",
        "with open('IDS_Assignment2/BigData_Files/output/Q6b_dfr.txt') as file:\n",
        "    file_reader = csv.reader(file, delimiter='\\t')\n",
        "    dfg = dict()\n",
        "    for row in file_reader:\n",
        "        _from,_to=row[0].split(',')\n",
        "        rel = (_from,_to)\n",
        "        freq = int(row[1])\n",
        "        dfg[rel] = freq\n",
        "\n",
        "# Visualize Directly-follows-graph (DFG)\n",
        "gviz = dfg_vis_factory.apply(dfg)\n",
        "dfg_vis_factory.view(gviz)\n",
        "dfg_vis_factory.save(gviz, \"dfg.png\")\n",
        "\n",
        "# Discover and Visualize Workflow-Net\n",
        "net, im, fm = dfg_mining_factory.apply(dfg)\n",
        "gviz = pn_vis_factory.apply(net, im, fm)\n",
        "pn_vis_factory.view(gviz)\n",
        "dfg_vis_factory.save(gviz, \"pt.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMza8tulXjQG"
      },
      "source": [
        "### Explanation:<br>\n",
        "For this task we create 2 mapper funtions and 2 reduce functions. Firstly these python files are uploaded to the docker container. We use `Patient` and `ModelTime` as Key for the first mapper and `Activity` as value. We use the merged file created in Q6a as input for the first MapReduce Task which uses mapper1 and reducer1 with Hadoop  streaming utility. The output of MapReduce 1 gives the traces from the event log. The output file can be found at **`BigData_Files\\output\\Q6b_DFG0\\part-00000`**.<br>\n",
        "The output of the first Mapreduce is then fed as input to the second MapReduce task which uses mapper2 and reducer2 with Hadoop streaming utility. The output of MapReduce 2 gives the directly-follows relations from the traces. The output file can be found at **`BigData_Files\\output\\Q6b_DFG-final\\part-00000`**. We then copy this file to our local computer from the hadoop system as a txt file(**`BigData_Files\\output\\Q6b_dfr.txt`**). This text file is then fed as input to the pm4py algorithm which visualizes the Directly-follows-graph (DFG) and discovers the process model for the event log.<br><br>\n",
        "All the python files and Hadoop/docker command's text file can be found at **`BigData_Files\\CodeFiles\\`** folder. The outputs of pm4py are at **`BigData_Files\\output\\`**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDfXPUEX44UT"
      },
      "source": [
        "### Performance Analysis\n",
        "\n",
        "(c) Calculate the total service time for each case using MapReduce algorithms. Explain how you calculate the total service time for each case with the following deliverables:\n",
        "\n",
        "    1) Mapper function (as python file(s))\n",
        "    2) Reducer function (as python file(s))\n",
        "    3) Hadoop commands for MapReduce calculation (as text file)\n",
        "    4) Result: total service times for cases (as text file)\n",
        "    \n",
        "Important! Please note that in this task, your result will be evaluated based on whether they are reproducible from your explanation. If you skip MapReduce calculations for this task, you will get 0 points.The deliverables of 1), 2), 3), and 4) should be submitted as outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAWEMFuSXoMm"
      },
      "source": [
        "### Explanation:\n",
        "Firtly we copy our mapper and reducer function python files to the docker container. Here we use only a single MapReduce task in the Hadoop streaming utility. The merged file from Q6a is used as input. The `Patient` case is used as key and `ServiceTime` as value for the mapper. The reducer funtion aggregates the Service time for each case. The output of the MapReduce task gives the Total service time per case. The output file is at **`BigData_Files\\output\\Q6c_TotServiceTime.txt`**.<br><br>\n",
        "All the python files and Hadoop/docker command's text file can be found at **`BigData_Files\\CodeFiles\\`** folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCCm_g_Q44UT"
      },
      "source": [
        "(d) Visualize 1000 cases with the longest total service time using any chart."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxDrHLqq44UU"
      },
      "source": [
        "# your code\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# input the result text file from Q6c\n",
        "dsTotServiceTime = pd.read_csv(r\"IDS_Assignment2/BigData_Files/output/Q6c_TotServiceTime.txt\",sep=\"\\t\",header=None,names=[\"Case\", \"Total_Service_Time\"])\n",
        "# sort the dataset based on Total_Service_Time column value\n",
        "dsTotServiceTime = dsTotServiceTime.sort_values(\"Total_Service_Time\",ascending=False)\n",
        "dsPlot = dsTotServiceTime[:1000] # take the 1000 longest total service time cases\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Distribution of Total_Service_Time\")\n",
        "sns.distplot(dsPlot['Total_Service_Time'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu02qpnrXtg_"
      },
      "source": [
        "### Explanation:<br>\n",
        "We use `sns.distplot` to visualize the 1000 cases with the longest total service. From the chart we can see that the peak density distribution is for total service time between 95000 and 96000. Thus a high number of cases have a total service time between the stated values.<br>\n",
        "The plot is also available as pdf at **`BigData_Files/output/Q6d_Plot.pdf`**."
      ]
    }
  ]
}